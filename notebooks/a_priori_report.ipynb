{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Priori Approach\n",
    "\n",
    "#### INTRODUCTION\n",
    "\n",
    "In this chapter we will explain one of the possible approach to the problem of identifying emotion in a photo or in a video.\n",
    "\n",
    "As soon as we received the task of the project we started trying to understand the domain in which we where going to work; during our researches we bumped into an interesting article talking about emotion recognition, written by Ruiz, Van de Weijer and Binefa “From Emotions to Action Units with Hidden and Semi-Hidden-Task Learning”, International Conference on Computer Vision (ICCV), 2015.\n",
    "\n",
    "In this paper, they investigate how the use of large databases labelled according to the 7 universal facial expressions (anger, disgust, fear, happiness, sadness, surprise, neutral), can increase the generalization ability of Action Unit classifiers. Using FACS (Facial Action Coding System), a system to taxonomize human facial movements by their appearance on the face, it is possible to code nearly any anatomically possible facial expression, deconstructing it into the specific Action Units (AU) that produced the expression. During the reading, we found the following object:\n",
    "<img src=\"tab.png\">\n",
    "\n",
    "This table shows Action Units  activation probability for each emotion according to the paper: \n",
    "P. Gosselin, G. Kirouac, and F. Y. Dor´e. \n",
    "“Components and recognition of facial expression in the communication of emotion by actors.” Journal of personality and social psychology, 1995.\n",
    "\n",
    "We decided consequently to exploit this table information for our project of identify emotions from video and/or pictures, but with a little variation on the concept.  Our idea is that the matter: “an AU has a probability to activate given an emotion” can be approximated by thinking that each basic emotion has a certain pattern that we, as humans, tend to recognize on another person by spotting some typical features of the pattern. The approximation lies in considering the most active features as the most recognizable patterns and consequently the most highlighted ones by a person: a person would tend on purpose or unintentionally to stress out more than other features, the ones that we associate with the emotions, i.e. the most activated ones. For example, for happiness AU6 and AU12 have the highest probability to activate based on the table. These action units are respectively cheek raiser and lip corner pulled. These two features movements are the ones that make a person immediately associate them with happiness. Therefore, we will approximate that by assuming that the intensity of these two Aus will be generally higher than the other ones, due to the fact that they are most recognizable features of the emotion and people would tend to stress them more.\n",
    "\n",
    "\n",
    "#### OPENFACE API\n",
    "\n",
    "We used Openface API to extract information about the AUs for each picture or video that we wanted to analyze. \n",
    "AUs can be described in two ways:\n",
    "\n",
    "Presence - if AU is visible in the face (for example AU01_c)\n",
    "Intensity - how intense is the AU (minimal to maximal) on a 5 point scale\n",
    "OpenFace provides both of these scores. For presence of AU 1 the column AU01_c in the output file would encode 0 as not present and 1 as present. For intensity of AU 1 the column AU01_r in the output file would range from 0 (not present), 1 (present at minimum intensity), 5 (present at maximum intensity), with continuous values in between.\n",
    "Unfortunately, the intensity and presence predictors have been trained separately and on slightly different datasets, this means that the predictions of both is not always consistent.\n",
    "\n",
    "Openface gives as output also a lot of other information such that the position of the face or in which direction the eyes are looking, as future development of this project one could try to exploit even that kind of information; in our approach we will use just the intensity of action units present in the matrix mentioned above.\n",
    "\n",
    "\n",
    "#### THE DATA \n",
    "\n",
    "The data we will use come from the Openface elaboration of a famous dataset, much used in a lot of works we read about, CK+.\n",
    "\n",
    "In this dataset there are plenty of images of faces, in particular in the dataset there are 123 subject, for each subject we have at least one sequence, 592 in total, and each sequence is made by at least 4 images in sequence where the first one is labeled as neutral and the last one as the maximum intensity of the emotion.\n",
    "This means that for every emotion we have the evolution of the face from the neutral  state to the emotion passing through different degree of emotion intensity.\n",
    "\n",
    "Anyway we will not use the whole images from the dataset but just the first, labeled as \"neutral\", and the last one of each sequence. \n",
    "\n",
    "A detailed report on the dataset we use for our analysis is given in \"Data_Analysis_Images_AUs\".\n",
    "\n",
    "\n",
    "\n",
    "#### OUR APPROACH\n",
    "\n",
    "In the article about emotion recognition, written by Ruiz, Van de Weijer and Binefa “From Emotions to Action Units with Hidden and Semi-Hidden-Task Learning”, form which the matrix above come from, there is no explanation of how the matrix could be used to go from AUs to emotion; it is presented just as a correlation matrix resulting from their studies.\n",
    "Anyway we tried to use it to make the converse way: from the AUs to the emotions. \n",
    "\n",
    "Our idea is to consider the matrix not as a correlation matrix but as a matrix made by 7 rows, each one express one emotion, but not in terms of probabilities of having the corresponding AUs activated, but as a real state in which the AUs should be to perfectly represent the emotion. So we consider each row as a vector, a point in $R^{14}$; now we are able to compute the distance between the \"theoretical emotion vector\" and our AUs vector.\n",
    "\n",
    "We will label each frame as the less distant emotion.\n",
    "\n",
    "\n",
    "\n",
    "#### ELABORATION PROCESS\n",
    "\n",
    "An output frame from a video is of the form:\n",
    "\n",
    "<img src=\"out.png\">\n",
    "\n",
    "For every frame we compute:\n",
    "\n",
    "* Normalize it: Every AU_r goes from 0 to 5, we scale it to [0,1], in order to be in the same scale of the matrix vectors.\n",
    "\n",
    "* Then we compute the vectorial distance from the frame to the vectoral configuration of each emotion (i.e. For example from the aforementioned table we have that sadness is associated with the vector [0.22, 0.01, 0.25, 0.00, 0.03, 0.39, 0.00, 0.05, 0.05, 0.00, 0.09,  0.17, 0.07, 0.00, 0.14, 0.20] etc..)\n",
    "\n",
    "* After computing the distance, we have as a result a number for each emotion, i.e. the distance of each emotion. The bigger it is, the more the emotion we are detecting is “far” from the emotion of the person corresponding to the frame we are considering.\n",
    "\n",
    "* We assume that the emotion in the frame is the one with matrix vectorial configuration less distant.\n",
    "\n",
    "\n",
    "#### IMPLEMENTATION\n",
    "\n",
    "We implemented the algorithm using Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Data manipulation **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start implementing the \"a priori matrix\" and importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#matrix data\n",
    "anger=pd.Series([0.17,0.10,0.33,0.25,0.03,0.05,0.00,0.1,0,0.05,0.06,0.4,0.31,0.49])\n",
    "disgust=pd.Series([0.01,0.01,0.35,0.01,0.06,0.36,0.06,0.21,0,0,0,0.25,0.32,0.4])\n",
    "fear=pd.Series([0.12,0.01,0.33,0.55,0,0.29,0.0,0.03,0,0.04,0.04,0.25,0.20,0.75])\n",
    "happyness=pd.Series([0.07,0.09,0.01,0.05,0.94,0.01,0.05,0.0,0.92,0.0,0.0,0.02,0.34,0.55])\n",
    "sadness=pd.Series([0.22,0.01,0.25,0.0,0.03,0.39,0.0,0.05,0.05,0.09,0.17,0.07,0.14,0.20])\n",
    "surprise=pd.Series([0.15,0.19,0.08,0.76,0.0,0.02,0.0,0.10,0.04,0.0,0.04,0.09,0.26,0.72])\n",
    "neutral=pd.Series([0,0,0,0,0,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "\n",
    "path=\"../data/labeled_fromvids.csv\"  #path to the data \n",
    "dat=pd.read_csv(path)#opening the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see from the matrix, the emotion are encoded in only 14 AUs, so we need to throw part of ours AUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#form the csv extracting the interest columns\n",
    "nam=dat.columns.values\n",
    "nam_fin=nam[2:18]\n",
    "nam_fin= nam_fin[nam_fin!=\"AU14_r\"]\n",
    "nam_fin= nam_fin[nam_fin!=\"AU23_r\"]\n",
    "opend=dat.iloc[:,2:18]\n",
    "del opend[\"AU14_r\"]\n",
    "del opend[\"AU23_r\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the data we noticed that there were some AUs which never take values different from zero, or other AUs take values different from zero just in few cases.\n",
    "In order to avoid distortion of the results we deleted the AUs which were always equal to zero or positive in less then 40% of the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "openda=opend.loc[:,((( opend>0).sum())/len(dat.iloc[:,0]) <0.60) ]\n",
    "opendat=openda.loc[:,openda.max()>0]\n",
    "dat_array=np.array(opendat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish the data manipulation we need to scale all our data to [0,1]. We will put all observation bigger than 3 to 3, this is to avoid the presence of outlier and to \"stretch\" the data in the interval [0,1], otherwise the presence of few observation bigger than 3 would affect badly our results. \n",
    "Finally we normalize each AUs to its maximum, in this way we avoid to have only observation near to zero where 3 is not reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_array=np.array(opendat)\n",
    "dat_array[(dat_array>3)]=3\n",
    "dati=dat_array/dat_array.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We steel have to adjust the matrix to the modification we made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat=np.zeros((7,14))\n",
    "mat[:]=[anger,disgust,fear,happyness,sadness,surprise,neutral]\n",
    "matrix=pd.DataFrame(mat, columns=nam_fin )\n",
    "\n",
    "matrix=matrix.loc[:,((opend>0).sum())/len(dat.iloc[:,0]) <0.60]\n",
    "matrix=matrix.loc[:,openda.max()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Distace **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run some experiments and understand which kind of distance would work better in term of accuracy we tested 200 different kind of distances. \n",
    "\n",
    "We used the Minkowski distance as reference and we tried to predict the emotion of a frame using from 1 to 201 degree of the distance and we iterated that to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.363914\n",
       "1      0.425076\n",
       "2      0.415902\n",
       "3      0.403670\n",
       "4      0.403670\n",
       "5      0.406728\n",
       "6      0.406728\n",
       "7      0.406728\n",
       "8      0.409786\n",
       "9      0.403670\n",
       "10     0.403670\n",
       "11     0.403670\n",
       "12     0.397554\n",
       "13     0.394495\n",
       "14     0.394495\n",
       "15     0.394495\n",
       "16     0.394495\n",
       "17     0.391437\n",
       "18     0.391437\n",
       "19     0.394495\n",
       "20     0.394495\n",
       "21     0.391437\n",
       "22     0.388379\n",
       "23     0.388379\n",
       "24     0.388379\n",
       "25     0.388379\n",
       "26     0.388379\n",
       "27     0.388379\n",
       "28     0.388379\n",
       "29     0.388379\n",
       "         ...   \n",
       "170    0.385321\n",
       "171    0.385321\n",
       "172    0.385321\n",
       "173    0.385321\n",
       "174    0.385321\n",
       "175    0.385321\n",
       "176    0.385321\n",
       "177    0.385321\n",
       "178    0.385321\n",
       "179    0.385321\n",
       "180    0.385321\n",
       "181    0.385321\n",
       "182    0.385321\n",
       "183    0.385321\n",
       "184    0.385321\n",
       "185    0.385321\n",
       "186    0.385321\n",
       "187    0.385321\n",
       "188    0.385321\n",
       "189    0.385321\n",
       "190    0.385321\n",
       "191    0.385321\n",
       "192    0.385321\n",
       "193    0.385321\n",
       "194    0.385321\n",
       "195    0.385321\n",
       "196    0.385321\n",
       "197    0.385321\n",
       "198    0.385321\n",
       "199    0.385321\n",
       "Length: 200, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is given as a comment because it takes some times to finish\n",
    "# to run, but it is how we took our conclusions.\n",
    "\n",
    "lis_ne=[]\n",
    "for v in range(200):\n",
    "    for i in range(len(dati[:,0])):\n",
    "        photo=dati[i,:]\n",
    "        lis_parz=[]\n",
    "        for u in range(7):\n",
    "            emo=matrix.loc[u,:]\n",
    "            dist=scipy.spatial.distance.minkowski(emo,photo,p=v+1)\n",
    "            lis_parz.append(dist)\n",
    "            \n",
    "        lis_ne.append(lis_parz)   \n",
    " \n",
    " \n",
    "ris_m=pd.DataFrame(lis_ne)        \n",
    "\n",
    "\n",
    "ris_m.T /ris_m.sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ris_m.columns=[\"anger\",\"disgust\",\"fear\",\"happyness\",\"sadness\",\"surprise\",\"neutral\"]\n",
    "\n",
    "temp=[1,3,4,5,6,7,0]\n",
    "emotions=pd.DataFrame(temp)\n",
    "\n",
    "\n",
    "\n",
    "clas=[]\n",
    "for i in range(len(ris_m.iloc[:,0])):\n",
    "    \n",
    "    m=ris_m.loc[i,:]==min(ris_m.loc[i,:])\n",
    "    rev=m.tolist()\n",
    "    rr=emotions[rev][0].tolist()[0]\n",
    "    clas.append(rr)\n",
    "\n",
    "papa_m=pd.Series(clas)\n",
    "\n",
    "\n",
    "\n",
    "derr=[]\n",
    "for i in range(200):\n",
    "    temp=papa_m[i*len(dati[:,0]):len(dati[:,0])+i*len(dati[:,0])]\n",
    "    temp.index=range(len(dati[:,0]))\n",
    "    a=(dat[\"emotion\"]==temp).sum() / len(dati[:,0])\n",
    "    derr.append(a)\n",
    "trend=pd.Series(derr)\n",
    "trend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we discovered running this script is that the one which works better is the Minkowski with 2 degrees, the Eucliden distance.\n",
    "\n",
    "So we will use the Euclidean distance to assign a label to a given frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EUCLIDEA\n",
    "\n",
    "#create a dataframe with the distances computed with the euclidean \n",
    "lis_fin=[]\n",
    "for i in range(len(dati[:,0])):\n",
    "    photo=dati[i,:]\n",
    "    lis_parz=[]\n",
    "    for u in range(7):\n",
    "        emo=matrix.loc[u,:]\n",
    "        dist=np.sqrt(sum((emo-photo)*(emo-photo)))\n",
    "        lis_parz.append(dist)\n",
    "        \n",
    "    lis_fin.append(lis_parz)   \n",
    "    \n",
    "ris=pd.DataFrame(lis_fin)        \n",
    "\n",
    "\n",
    "\n",
    "ris.columns=[\"anger\",\"disgust\",\"fear\",\"happyness\",\"sadness\",\"surprise\",\"neutral\"]\n",
    "\n",
    "\n",
    "temp=[1,3,4,5,6,7,0]\n",
    "emotions=pd.DataFrame(temp)\n",
    "\n",
    "a=ris.iloc[325,:].tolist()\n",
    "a.sort()\n",
    "c=np.array(temp)\n",
    "c[a[0]==ris.iloc[325,:]][0]\n",
    "\n",
    "#selecting the minimal distance between each photo and the emotions\n",
    "clas=[]\n",
    "for i in range(len(ris.iloc[:,0])):\n",
    "    a=ris.iloc[i,:].tolist()\n",
    "    a.sort()\n",
    "    c=np.array(temp)\n",
    "    q=c[a[0]==ris.iloc[i,:]][0]\n",
    "\n",
    "    clas.append(q)\n",
    "\n",
    "papa=pd.Series(clas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can say that with this approach we reached an accuracy of more than 42%.\n",
    "Thinking that giving at random a label we should have an accuracy of 14% we can consider it a great results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42507645259938837"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percentage of right clissified emotions\n",
    "((dat[\"emotion\"]==papa).sum())/327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESULTS AND COMMENTS\n",
    "\n",
    "\n",
    "We reached an accuracy of 42.51% , so we can assume that the table describes quite well the activation of AUs with respect of the emotions and our approximation has some true foundation. In the other hand, the accuracy reached is very sensitive to the dataset we used due to the approach we had. As explained before, the implementation included some editing based on the dataset conformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUTURE POSSIBLE IMPROVEMENTS\n",
    "\n",
    "Due to the accuracy being sensitive to the dataset in our case, it would be advisable to try this Matrix Approach to multiple datasets and foresee if the matrix and the approximation idea can be considered as good and useful general tools for emotions detection. Other possible improvements can be try even more different distances or other scaling approaches. Finally, Openface was not producing values for all AUs, so it could be useful to try the algorithm to other APIs that produce even more AUs and check how the accuracy may change.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
